{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow Data Validation\n",
    "\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "1. Review TFDV methods.\n",
    "2. Generate statistics.\n",
    "3. Visualize statistics.\n",
    "4. Infer a schema.\n",
    "5. Update a schema.\n",
    "\n",
    "\n",
    "\n",
    "## Introduction \n",
    "This lab is an introduction to TensorFlow Data Validation (TFDV), a key component of TensorFlow Extended.  This lab serves as a foundation for understanding the features of TFDV and how it can help you understand, validate, and monitor your data. \n",
    "\n",
    "TFDV can be used for generating schemas and statistics about the distribution of every feature in the dataset. Such information is useful for comparing multiple datasets (e.g. training vs inference datasets) and reporting:\n",
    "\n",
    "Statistical differences in the features distribution\n",
    "TFDV also offers visualization capabilities for comparing datasets based on the Google PAIR Facets project.  \n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in the [student lab notebook](../labs/tfdv_basic_spending.ipynb) -- try to complete that notebook first before reviewing this solution notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsHg6SD2nO1v"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.9/site-packages (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from pyarrow) (1.19.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.19.5)\n",
      "Collecting tensorflow-data-validation\n",
      "  Obtaining dependency information for tensorflow-data-validation from https://files.pythonhosted.org/packages/08/45/c14a8857b2fc0a522991d3914e860f8f0bdf3ee33bcf67077eb397e35af4/tensorflow_data_validation-1.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow_data_validation-1.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /opt/conda/lib/python3.9/site-packages (from tensorflow-data-validation) (0.15.0)\n",
      "Collecting apache-beam[gcp]<3,>=2.47 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for apache-beam[gcp]<3,>=2.47 from https://files.pythonhosted.org/packages/9c/83/5eeca83114066910e567750ea71e4a92a0be9bf2d937beec29eb718be41b/apache_beam-2.50.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading apache_beam-2.50.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-data-validation) (1.3.2)\n",
      "Collecting numpy>=1.22.0 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for numpy>=1.22.0 from https://files.pythonhosted.org/packages/75/cd/7ae0f2cd3fc68aea6cfb2b7e523842e1fa953adb38efabc110d27ba6e423/numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<2,>=1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-data-validation) (1.4.4)\n",
      "Collecting protobuf<5,>=3.20.3 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for protobuf<5,>=3.20.3 from https://files.pythonhosted.org/packages/bb/c3/6a06208ecf0934ecaf509b51c52a6cf688586f54ae81ac65c56124571494/protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting pyarrow<11,>=10 (from tensorflow-data-validation)\n",
      "  Downloading pyarrow-10.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyfarmhash<0.4,>=0.2.2 (from tensorflow-data-validation)\n",
      "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /opt/conda/lib/python3.9/site-packages (from tensorflow-data-validation) (1.15.0)\n",
      "Collecting tensorflow<3,>=2.13 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorflow<3,>=2.13 from https://files.pythonhosted.org/packages/81/16/3aaaf911d8309b9afb29bff97e819c52b011d4ab184c7b01cec92abd018a/tensorflow-2.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tensorflow-metadata<1.15,>=1.14.0 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorflow-metadata<1.15,>=1.14.0 from https://files.pythonhosted.org/packages/41/23/3705c7139886c079ef4c0e3be56a5a1fb90e9ee413a4b7caaee0ee0ea6fe/tensorflow_metadata-1.14.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tfx-bsl<1.15,>=1.14.0 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for tfx-bsl<1.15,>=1.14.0 from https://files.pythonhosted.org/packages/6b/dd/2b5f2ab59ae16b41c8fb42aff42009d8681651a545faa856fa22ab84d7f0/tfx_bsl-1.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tfx_bsl-1.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.7)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.9.5)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.8.2)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.18)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.48.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.7.2)\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.21.0)\n",
      "Collecting numpy>=1.22.0 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for numpy>=1.22.0 from https://files.pythonhosted.org/packages/7a/7c/d7b2a0417af6428440c0ad7cb9799073e507b1a465f827d058b826236964/numpy-1.24.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.24.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.6.1)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.13.0)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.22.3)\n",
      "Collecting protobuf<5,>=3.20.3 (from tensorflow-data-validation)\n",
      "  Obtaining dependency information for protobuf<5,>=3.20.3 from https://files.pythonhosted.org/packages/01/cb/445b3e465abdb8042a41957dc8f60c54620dc7540dbcf9b458a921531ca2/protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2023.3)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2023.8.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.10.0.2)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.19.0)\n",
      "Requirement already satisfied: cachetools<6,>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.2.4)\n",
      "Collecting google-api-core<3,>=2.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-api-core<3,>=2.0.0 from https://files.pythonhosted.org/packages/4d/ce/4fd62ea66b3508debc795e475336ce915929765870f0ad52328426ba016e/google_api_core-2.12.0-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.5.31)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.35.0)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.1.0)\n",
      "Collecting google-cloud-datastore<3,>=2.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-datastore<3,>=2.0.0 from https://files.pythonhosted.org/packages/e6/e3/9e19e26b4fc61aa64a73d097c9e5ce3b4cbb62d7d0ea2c0554b2105b54d6/google_cloud_datastore-2.18.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_datastore-2.18.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.18.3)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.8.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.11.4)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.16.2)\n",
      "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.3.3)\n",
      "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-bigtable<3,>=2.19.0 from https://files.pythonhosted.org/packages/f1/2e/fcdd3aed46825c04bc93c0223fff674059e6a789cfda5374948bea1a3f6e/google_cloud_bigtable-2.21.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_bigtable-2.21.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.40.1)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.12.2)\n",
      "Collecting google-cloud-language<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-language<3,>=2.0 from https://files.pythonhosted.org/packages/19/4c/8a42006ec29f26d1d0f47f5fdebe7138a700eda8e3668d53ffcb55c9a774/google_cloud_language-2.11.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_language-2.11.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-cloud-videointelligence<3,>=2.0 from https://files.pythonhosted.org/packages/ca/0e/ce99f6c36d5e7ccbc5827d714a947da5b864a30e060f2e5dddad4166f663/google_cloud_videointelligence-2.11.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_videointelligence-2.11.4-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: google-cloud-vision<4,>=2 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.4.4)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.7.1)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /opt/conda/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.31.0)\n",
      "Collecting absl-py<2.0.0,>=0.9 (from tensorflow-data-validation)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow<3,>=2.13->tensorflow-data-validation) (1.6.3)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow<3,>=2.13->tensorflow-data-validation) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow<3,>=2.13->tensorflow-data-validation) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow<3,>=2.13->tensorflow-data-validation) (3.1.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/7b/be/4b211a4e432502c432e3077aa66b0d64f6d7cb4c36613d65c49d9b799919/ml_dtypes-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow<3,>=2.13->tensorflow-data-validation) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow<3,>=2.13->tensorflow-data-validation) (23.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow<3,>=2.13->tensorflow-data-validation) (68.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow<3,>=2.13->tensorflow-data-validation) (1.1.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow<3,>=2.13->tensorflow-data-validation) (1.12.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/49/49/fa8224d5da4c81c958503c159bae176cae329f8704bef881d8fd7d99a180/tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-metadata<1.15,>=1.14.0->tensorflow-data-validation) (1.60.0)\n",
      "Collecting protobuf<5,>=3.20.3 (from tensorflow-data-validation)\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.11 in /opt/conda/lib/python3.9/site-packages (from tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation) (1.8.0)\n",
      "Collecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorflow-serving-api<3,>=2.13.0 from https://files.pythonhosted.org/packages/71/fe/4004906ca558255673fab41a1bb2189ca6454417487a8608100677c8cc15/tensorflow_serving_api-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_serving_api-2.13.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<3,>=2.13->tensorflow-data-validation) (0.41.2)\n",
      "Collecting google-auth<3,>=1.18.0 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for google-auth<3,>=1.18.0 from https://files.pythonhosted.org/packages/d7/88/1826b0c047c48763b36ed854a984127b430a16b70003155d7b19975f1d59/google_auth-2.23.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of google-api-python-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-python-client<2,>=1.7.11 (from tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation)\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.15,>=1.14.0->tensorflow-data-validation) (3.0.1)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.9/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.1.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (4.9)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.9/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.34.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.9/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.10.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.9/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.10.3)\n",
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.8.5.post1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2.5.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.9/site-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.12.6)\n",
      "Collecting grpcio!=1.48.0,<2,>=1.33.1 (from apache-beam[gcp]<3,>=2.47->tensorflow-data-validation)\n",
      "  Obtaining dependency information for grpcio!=1.48.0,<2,>=1.33.1 from https://files.pythonhosted.org/packages/5a/e8/c6bbb12ec726c80c05baaab5dc0d026798ef6e9f53edd7a3798655b9c242/grpcio-1.58.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.58.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: grpcio-status>=1.33.2 in /opt/conda/lib/python3.9/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.48.1)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /opt/conda/lib/python3.9/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (6.5.0)\n",
      "Requirement already satisfied: sqlparse>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.4.4)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.9/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.9/site-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (2023.7.22)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow<3,>=2.13->tensorflow-data-validation) (3.4.4)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow<3,>=2.13->tensorflow-data-validation)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow<3,>=2.13->tensorflow-data-validation) (2.3.7)\n",
      "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<3,>=2.13->tensorflow-data-validation) (1.3.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (1.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow<3,>=2.13->tensorflow-data-validation) (6.8.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.9/site-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow-data-validation) (0.4.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<3,>=2.13->tensorflow-data-validation) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow<3,>=2.13->tensorflow-data-validation) (3.16.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<3,>=2.13->tensorflow-data-validation) (3.2.2)\n",
      "Downloading tensorflow_data_validation-1.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Downloading tfx_bsl-1.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading google_api_core-2.12.0-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_bigtable-2.21.0-py2.py3-none-any.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.0/293.0 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_datastore-2.18.0-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.3/177.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_language-2.11.1-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_videointelligence-2.11.4-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.58.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_serving_api-2.13.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading apache_beam-2.50.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyfarmhash\n",
      "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp39-cp39-linux_x86_64.whl size=13872 sha256=24e859e7d3c560797c51a2d1a37ca982cdf2586df1134c2a6e94018a50f8ba05\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/de/2b/b1/c541160670d70f4b08c4786f4e155337d4baeaa3e01d9d1400\n",
      "Successfully built pyfarmhash\n",
      "Installing collected packages: pyfarmhash, libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, protobuf, numpy, keras, grpcio, absl-py, pyarrow, ml-dtypes, google-auth, tensorflow-metadata, google-auth-oauthlib, google-api-core, apache-beam, tensorboard, google-api-python-client, tensorflow, google-cloud-videointelligence, google-cloud-language, google-cloud-datastore, google-cloud-bigtable, tensorflow-serving-api, tfx-bsl, tensorflow-data-validation\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.9 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script plasma_store is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tensorboard is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.11 which is incompatible.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.4 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.14.0 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.34.0 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires tensorflow-metadata<0.15,>=0.14, but you have tensorflow-metadata 1.14.0 which is incompatible.\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 apache-beam-2.50.0 flatbuffers-23.5.26 google-api-core-2.12.0 google-api-python-client-1.12.11 google-auth-2.23.2 google-auth-oauthlib-1.0.0 google-cloud-bigtable-2.21.0 google-cloud-datastore-2.18.0 google-cloud-language-2.11.1 google-cloud-videointelligence-2.11.4 grpcio-1.58.0 keras-2.14.0 libclang-16.0.6 ml-dtypes-0.2.0 numpy-1.24.4 protobuf-3.20.3 pyarrow-10.0.1 pyfarmhash-0.3.2 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tensorflow-2.14.0 tensorflow-data-validation-1.14.0 tensorflow-estimator-2.14.0 tensorflow-io-gcs-filesystem-0.34.0 tensorflow-metadata-1.14.0 tensorflow-serving-api-2.13.0 tfx-bsl-1.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install numpy\n",
    "!pip install tensorflow-data-validation --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsHg6SD2nO1v"
   },
   "source": [
    "**Restart the kernel (Kernel > Restart kernel > Restart).**\n",
    "\n",
    "**Re-run the above cell and proceed further.**\n",
    "\n",
    "**Note: Please ignore any incompatibility warnings and errors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "2023-09-29 15:08:09.987516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-09-29 15:08:09.987597: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-09-29 15:08:09.987640: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-29 15:08:10.002573: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing TensorFlow Data Validation\n",
      "TFDV version: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_data_validation as tfdv\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Installing TensorFlow Data Validation')\n",
    "!pip install -q tensorflow_data_validation[visualization]\n",
    "\n",
    "print('TFDV version: {}'.format(tfdv.version.__version__))\n",
    "# Confirm that we're using Python 3\n",
    "assert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fnm6Mj3vTGLm"
   },
   "source": [
    "###  Load the Consumer Spending Dataset\n",
    "\n",
    "We will download our dataset from Google Cloud Storage. The columns in the dataset are:\n",
    "\n",
    "* 'Graduated': Whether or not the person is a college graduate\n",
    "* 'Work Experience': The number of years in the workforce\n",
    "* 'Family Size': The size of the family unit\n",
    "* 'Spending Score': The spending score for consumer spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Spending_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Graduated     Profession  Work_Experience  Family_Size Spending_Score\n",
       "0        No     Healthcare              1.0          4.0            Low\n",
       "1       Yes       Engineer              NaN          3.0        Average\n",
       "2       Yes       Engineer              1.0          1.0            Low\n",
       "3       Yes         Lawyer              0.0          2.0           High\n",
       "4       Yes  Entertainment              NaN          6.0           High"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a comma-separated values (csv) file into DataFrame.\n",
    "# TODO: Your code goes here\n",
    "score_train = pd.read_csv('data/score_train.csv')\n",
    "score_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Spending_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Executive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Artist</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Graduated     Profession  Work_Experience  Family_Size Spending_Score\n",
       "0        No         Doctor              0.0          5.0        Average\n",
       "1       Yes  Entertainment              1.0          4.0        Average\n",
       "2        No         Lawyer              0.0          5.0            Low\n",
       "3       Yes      Executive              1.0          5.0           High\n",
       "4       Yes         Artist              1.0          2.0        Average"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a comma-separated values (csv) file into DataFrame.\n",
    "# TODO: Your code goes here\n",
    "score_test = pd.read_csv('data/score_test.csv')\n",
    "score_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Graduated        3964 non-null   object \n",
      " 1   Profession       3944 non-null   object \n",
      " 2   Work_Experience  3589 non-null   float64\n",
      " 3   Family_Size      3831 non-null   float64\n",
      " 4   Spending_Score   4000 non-null   object \n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "score_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the methods present in TFDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CombinerStatsGenerator',\n",
       " 'CrossFeatureView',\n",
       " 'DatasetListView',\n",
       " 'DatasetView',\n",
       " 'DetectFeatureSkew',\n",
       " 'FeaturePath',\n",
       " 'FeatureView',\n",
       " 'GenerateStatistics',\n",
       " 'MergeDatasetFeatureStatisticsList',\n",
       " 'StatsOptions',\n",
       " 'TransformStatsGenerator',\n",
       " 'WriteStatisticsToBinaryFile',\n",
       " 'WriteStatisticsToRecordsAndBinaryFile',\n",
       " 'WriteStatisticsToTFRecord',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'anomalies',\n",
       " 'api',\n",
       " 'arrow',\n",
       " 'coders',\n",
       " 'compare_slices',\n",
       " 'constants',\n",
       " 'default_sharded_output_suffix',\n",
       " 'default_sharded_output_supported',\n",
       " 'display_anomalies',\n",
       " 'display_schema',\n",
       " 'experimental_get_feature_value_slicer',\n",
       " 'generate_dummy_schema_with_paths',\n",
       " 'generate_statistics_from_csv',\n",
       " 'generate_statistics_from_dataframe',\n",
       " 'generate_statistics_from_tfrecord',\n",
       " 'get_confusion_count_dataframes',\n",
       " 'get_domain',\n",
       " 'get_feature',\n",
       " 'get_feature_stats',\n",
       " 'get_match_stats_dataframe',\n",
       " 'get_skew_result_dataframe',\n",
       " 'get_slice_stats',\n",
       " 'get_statistics_html',\n",
       " 'infer_schema',\n",
       " 'load_anomalies_text',\n",
       " 'load_schema_text',\n",
       " 'load_sharded_statistics',\n",
       " 'load_statistics',\n",
       " 'load_stats_binary',\n",
       " 'load_stats_text',\n",
       " 'pywrap',\n",
       " 'set_domain',\n",
       " 'skew',\n",
       " 'statistics',\n",
       " 'types',\n",
       " 'update_schema',\n",
       " 'utils',\n",
       " 'validate_corresponding_slices',\n",
       " 'validate_examples_in_csv',\n",
       " 'validate_examples_in_tfrecord',\n",
       " 'validate_statistics',\n",
       " 'version',\n",
       " 'visualize_statistics',\n",
       " 'write_anomalies_text',\n",
       " 'write_schema_text',\n",
       " 'write_stats_text']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check methods present in tfdv\n",
    "# TODO: Your code goes here\n",
    "[methods for methods in dir(tfdv)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing data with TFDV\n",
    "The usual workflow when using TFDV during training is as follows:\n",
    "\n",
    "\n",
    "1.   Generate statistics for the data\n",
    "2.   Use those statistics to generate a schema for each feature\n",
    "3.   Visualize the schema and statistics and manually inspect them\n",
    "4.   Update the schema if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and visualize statistics\n",
    "\n",
    "First we'll use [`tfdv.generate_statistics_from_csv`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv) to compute statistics for our training data. (ignore the snappy warnings)\n",
    "\n",
    "TFDV can compute descriptive [statistics](https://github.com/tensorflow/metadata/blob/v0.6.0/tensorflow_metadata/proto/v0/statistics.proto) that provide a quick overview of the data in terms of the features that are present and the shapes of their value distributions.\n",
    "\n",
    "Internally, TFDV uses [Apache Beam](https://beam.apache.org/)'s data-parallel processing framework to scale the computation of statistics over large datasets. For applications that wish to integrate deeper with TFDV (e.g., attach statistics generation at the end of a data-generation pipeline), the API also exposes a Beam PTransform for statistics generation.\n",
    "\n",
    "**NOTE:  Compute statistics**\n",
    "* [tfdv.generate_statistics_from_csv](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv)\n",
    "* [tfdv.generate_statistics_from_dataframe](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_dataframe)\n",
    "* [tfdv.generate_statistics_from_tfrecord](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_tfrecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Statistics from a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute data statistics for the input pandas DataFrame.\n",
    "# TODO: Your code goes here\n",
    "stats = tfdv.generate_statistics_from_dataframe(dataframe=score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use [`tfdv.visualize_statistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics), which uses [Facets](https://pair-code.github.io/facets/) to create a succinct visualization of our training data:\n",
    "\n",
    "* Notice that numeric features and categorical features are visualized separately, and that charts are displayed showing the distributions for each feature.\n",
    "* Notice that features with missing or zero values display a percentage in red as a visual indicator that there may be issues with examples in those features.  The percentage is the percentage of examples that have missing or zero values for that feature.\n",
    "* Notice that there are no examples with values for `pickup_census_tract`.  This is an opportunity for dimensionality reduction!\n",
    "* Try clicking \"expand\" above the charts to change the display\n",
    "* Try hovering over bars in the charts to display bucket ranges and counts\n",
    "* Try switching between the log and linear scales, and notice how the log scale reveals much more detail about the `payment_type` categorical feature\n",
    "* Try selecting \"quantiles\" from the \"Chart to show\" menu, and hover over the markers to show the quantile percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the input statistics using Facets.\n",
    "# TODO: Your code goes here\n",
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFDV generates different types of statistics based on the type of features.\n",
    "\n",
    "**For numerical features, TFDV computes for every feature:**\n",
    "* Count of records\n",
    "* Number of missing (i.e. null values)\n",
    "* Histogram of values\n",
    "* Mean and standard deviation\n",
    "* Minimum and maximum values\n",
    "* Percentage of zero values\n",
    "\n",
    "**For categorical features, TFDV provides:**\n",
    "* Count of values\n",
    "* Percentage of missing values\n",
    "* Number of unique values\n",
    "* Average string length\n",
    "* Count for each label and its rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the score_train and the score_test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_stats = tfdv.generate_statistics_from_dataframe(dataframe=score_train)\n",
    "test_stats = tfdv.generate_statistics_from_dataframe(dataframe=score_test)\n",
    "\n",
    "tfdv.visualize_statistics(\n",
    "  lhs_statistics=train_stats, lhs_name='TRAIN_DATASET',\n",
    "  rhs_statistics=test_stats, rhs_name='NEW_DATASET')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVR02-y4V0uM"
   },
   "source": [
    "### Infer a schema\n",
    "\n",
    "Now let's use [`tfdv.infer_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) to create a schema for our data.  A schema defines constraints for the data that are relevant for ML. Example constraints include the data type of each feature, whether it's numerical or categorical, or the frequency of its presence in the data.  For categorical features the schema also defines the domain - the list of acceptable values.  Since writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics.\n",
    "\n",
    "Getting the schema right is important because the rest of our production pipeline will be relying on the schema that TFDV generates to be correct.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Schema\n",
    "Once statistics are generated, the next step is to generate a schema for our dataset. This schema will map each feature in the dataset to a type (float, bytes, etc.). Also define feature boundaries (min, max, distribution of values and missings, etc.).\n",
    "\n",
    "Link to infer schema\n",
    "https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema\n",
    "\n",
    "With TFDV, we generate schema from statistics using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6LLkRJThVr9m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"Graduated\"\n",
      "  value_count {\n",
      "    min: 1\n",
      "    max: 1\n",
      "  }\n",
      "  type: BYTES\n",
      "  domain: \"Graduated\"\n",
      "  presence {\n",
      "    min_count: 1\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"Profession\"\n",
      "  value_count {\n",
      "    min: 1\n",
      "    max: 1\n",
      "  }\n",
      "  type: BYTES\n",
      "  domain: \"Profession\"\n",
      "  presence {\n",
      "    min_count: 1\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"Work_Experience\"\n",
      "  value_count {\n",
      "    min: 1\n",
      "    max: 1\n",
      "  }\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_count: 1\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"Family_Size\"\n",
      "  value_count {\n",
      "    min: 1\n",
      "    max: 1\n",
      "  }\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_count: 1\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"Spending_Score\"\n",
      "  type: BYTES\n",
      "  domain: \"Spending_Score\"\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "string_domain {\n",
      "  name: \"Graduated\"\n",
      "  value: \"No\"\n",
      "  value: \"Yes\"\n",
      "}\n",
      "string_domain {\n",
      "  name: \"Profession\"\n",
      "  value: \"Artist\"\n",
      "  value: \"Doctor\"\n",
      "  value: \"Engineer\"\n",
      "  value: \"Entertainment\"\n",
      "  value: \"Executive\"\n",
      "  value: \"Healthcare\"\n",
      "  value: \"Homemaker\"\n",
      "  value: \"Lawyer\"\n",
      "  value: \"Marketing\"\n",
      "}\n",
      "string_domain {\n",
      "  name: \"Spending_Score\"\n",
      "  value: \"Average\"\n",
      "  value: \"High\"\n",
      "  value: \"Low\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Infers schema from the input statistics.\n",
    "# TODO: Your code goes here\n",
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema also provides documentation for the data, and so is useful when different developers work on the same data.  Let's use [`tfdv.display_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema) to display the inferred schema so that we can review it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFDV provides a API to print a summary of each feature schema using\n",
    "\n",
    "In this visualization, the columns stand for:\n",
    "\n",
    "**Type** indicates the feature datatype.\n",
    "\n",
    "**Presence** indicates whether the feature must be present in 100% of examples (required) or not (optional).\n",
    "\n",
    "**Valency** indicates the number of values required per training example. \n",
    "\n",
    "**Domain and Values** indicates The feature domain and its values\n",
    "\n",
    "In the case of categorical features, single indicates that each training example must have exactly one category for the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Schema \n",
    "As stated above, **Presence** indicates whether the feature must be present in 100% of examples (required) or not (optional).  Currently, all of our features except for our target label are shown as \"optional\". We need to make our features all required except for \"Work Experience\".  We will need to update the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFDV lets you update the schema according to your domain knowledge of the data if you are not satisfied by the auto-generated schema.  We will update three use cases:  Making a feature required, adding a value to a feature, and change a feature from a float to an integer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change optional features to required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Family_Size from FLOAT to Int\n",
    "Graduated_feature = tfdv.get_feature(schema, 'Graduated')\n",
    "Graduated_feature.presence.min_fraction = 1.0\n",
    "Profession_feature = tfdv.get_feature(schema, 'Profession')\n",
    "Profession_feature.presence.min_fraction = 1.0\n",
    "Family_Size_feature = tfdv.get_feature(schema, 'Family_Size')\n",
    "Family_Size_feature.presence.min_fraction = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update a feature with a new value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add \"self-employed\" to the Profession feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profession_domain = tfdv.get_domain(schema, 'Profession')\n",
    "Profession_domain.value.insert(0, 'Self-Employed')\n",
    "Profession_domain.value\n",
    "# [0 indicates I want 'Self-Employed to come first', if the number were 3, \n",
    "# it would be placed after the third value. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's remove \"Homemaker\" from \"Profession\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profession_domain = tfdv.get_domain(schema, 'Profession')\n",
    "Profession_domain.value.remove('Homemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profession_domain.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change a feature from a float to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Family_Size to Int\n",
    "size = tfdv.get_feature(schema, 'Family_Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size.type=2\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lab, you compare two datasets and check for anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8eC59yISdGB"
   },
   "source": [
    "## When to use TFDV\n",
    "\n",
    "It's easy to think of TFDV as only applying to the start of your training pipeline, as we did here, but in fact it has many uses. Here are a few more:\n",
    "\n",
    "* Validating new data for inference to make sure that we haven't suddenly started receiving bad features\n",
    "* Validating new data for inference to make sure that our model has trained on that part of the decision surface\n",
    "* Validating our data after we've transformed it and done feature engineering (probably using [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/transform)) to make sure we haven't done something wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/GoogleCloudPlatform/mlops-on-gcp/blob/master/examples/tfdv-structured-data/tfdv-covertype.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tghWegsjhpkt"
   ],
   "name": "tfdv_basic.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
